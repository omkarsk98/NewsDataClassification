{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsDataClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarsk98/NewsDataClassification/blob/development/NewsDataClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtVE_u-I0m91",
        "colab_type": "code",
        "outputId": "a34b67e2-86bb-4faa-a90c-9d4bd18ecb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/omkarsk98/NewsDataClassification.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NewsDataClassification'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/50)\u001b[K\rremote: Counting objects:   4% (2/50)\u001b[K\rremote: Counting objects:   6% (3/50)\u001b[K\rremote: Counting objects:   8% (4/50)\u001b[K\rremote: Counting objects:  10% (5/50)\u001b[K\rremote: Counting objects:  12% (6/50)\u001b[K\rremote: Counting objects:  14% (7/50)\u001b[K\rremote: Counting objects:  16% (8/50)\u001b[K\rremote: Counting objects:  18% (9/50)\u001b[K\rremote: Counting objects:  20% (10/50)\u001b[K\rremote: Counting objects:  22% (11/50)\u001b[K\rremote: Counting objects:  24% (12/50)\u001b[K\rremote: Counting objects:  26% (13/50)\u001b[K\rremote: Counting objects:  28% (14/50)\u001b[K\rremote: Counting objects:  30% (15/50)\u001b[K\rremote: Counting objects:  32% (16/50)\u001b[K\rremote: Counting objects:  34% (17/50)\u001b[K\rremote: Counting objects:  36% (18/50)\u001b[K\rremote: Counting objects:  38% (19/50)\u001b[K\rremote: Counting objects:  40% (20/50)\u001b[K\rremote: Counting objects:  42% (21/50)\u001b[K\rremote: Counting objects:  44% (22/50)\u001b[K\rremote: Counting objects:  46% (23/50)\u001b[K\rremote: Counting objects:  48% (24/50)\u001b[K\rremote: Counting objects:  50% (25/50)\u001b[K\rremote: Counting objects:  52% (26/50)\u001b[K\rremote: Counting objects:  54% (27/50)\u001b[K\rremote: Counting objects:  56% (28/50)\u001b[K\rremote: Counting objects:  58% (29/50)\u001b[K\rremote: Counting objects:  60% (30/50)\u001b[K\rremote: Counting objects:  62% (31/50)\u001b[K\rremote: Counting objects:  64% (32/50)\u001b[K\rremote: Counting objects:  66% (33/50)\u001b[K\rremote: Counting objects:  68% (34/50)\u001b[K\rremote: Counting objects:  70% (35/50)\u001b[K\rremote: Counting objects:  72% (36/50)\u001b[K\rremote: Counting objects:  74% (37/50)\u001b[K\rremote: Counting objects:  76% (38/50)\u001b[K\rremote: Counting objects:  78% (39/50)\u001b[K\rremote: Counting objects:  80% (40/50)\u001b[K\rremote: Counting objects:  82% (41/50)\u001b[K\rremote: Counting objects:  84% (42/50)\u001b[K\rremote: Counting objects:  86% (43/50)\u001b[K\rremote: Counting objects:  88% (44/50)\u001b[K\rremote: Counting objects:  90% (45/50)\u001b[K\rremote: Counting objects:  92% (46/50)\u001b[K\rremote: Counting objects:  94% (47/50)\u001b[K\rremote: Counting objects:  96% (48/50)\u001b[K\rremote: Counting objects:  98% (49/50)\u001b[K\rremote: Counting objects: 100% (50/50)\u001b[K\rremote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 50 (delta 22), reused 15 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8wVHW5d00z1",
        "colab_type": "code",
        "outputId": "d77c48dc-b3bc-4d42-e1c9-190f3c8a9d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd NewsDataClassification"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NewsDataClassification/NewsDataClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ivy1jcg1KVc",
        "colab_type": "code",
        "outputId": "e1e4a943-6a54-4e31-8444-2104aac38d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!git checkout development"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'development' set up to track remote branch 'development' from 'origin'.\n",
            "Switched to a new branch 'development'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTChM-bd_umC",
        "colab_type": "code",
        "outputId": "deb51a96-9e2e-4f46-a533-c45b15b975cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-10 09:00:56--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.137.110\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.137.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  42.6MB/s    in 30s     \n",
            "\n",
            "2019-10-10 09:01:26 (52.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV1FV6q2n61M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9-41Dib1B-w",
        "colab_type": "code",
        "outputId": "c14f6990-6225-43d6-e2e9-2f019e37eb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin  News_Final.csv\n",
            "index.py\t\t\t    NormalisedData.csv\n",
            "NewsDataClassification.ipynb\t    README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJDZ2gA83IWg",
        "colab_type": "code",
        "outputId": "5866a08b-993f-444c-e4dc-d8c7e31c4fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ozUYJ3oXO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from gensim import models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score ,confusion_matrix\n",
        "import time\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70GDVH6tL1r",
        "colab_type": "code",
        "outputId": "54ff8bae-8b36-4052-afaa-913a2ec3ea7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# read csv\n",
        "main_data = pd.read_csv('News_Final.csv')\n",
        "# read titles from it\n",
        "article_titles = main_data['TITLE']\n",
        "labels = main_data[\"CATEGORY\"]\n",
        "\n",
        "# Create a list of strings, one for each title\n",
        "titles_list = [title for title in article_titles]\n",
        "# form a single string fro the list of strings\n",
        "big_title_string = ' '.join(titles_list)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WqUDhgQJaIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the string into words\n",
        "tokens = word_tokenize(big_title_string)\n",
        "\n",
        "# Remove non-alphabetic tokens, such as punctuation\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if not word in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55YAVl-Oua1m",
        "colab_type": "code",
        "outputId": "935efa45-d624-45cf-b1a7-802d9ceb9d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Load word2vec model (trained on Google's corpus)\n",
        "model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n",
        "# Check dimension of word vectors\n",
        "print(\"Dimensions of the model\",model.vector_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dimensions of the model 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_h-pzrEzhNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using some pre built functions\n",
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in model.vocab]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
        "# Here, we need each document to remain a document \n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# Function that will help us drop documents that have no word vectors in word2vec\n",
        "def has_vector_representation(word2vec_model, doc):\n",
        "    \"\"\"check if at least one word of the document is in the\n",
        "    word2vec dictionary\"\"\"\n",
        "    return not all(word not in word2vec_model.vocab for word in doc)\n",
        "\n",
        "# Filter out documents\n",
        "def filter_docs(corpus, texts, labels, condition_on_doc):\n",
        "    \"\"\"\n",
        "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
        "    \"\"\"\n",
        "    number_of_docs = len(corpus)\n",
        "    \n",
        "    if texts is not None:\n",
        "        texts = [text for (text, doc) in zip(texts, corpus)\n",
        "                 if condition_on_doc(doc)]\n",
        "    \n",
        "\n",
        "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
        "    \n",
        "    final_labels = []\n",
        "    for i in range(len(corpus)):\n",
        "      if condition_on_doc(corpus[i]):\n",
        "        final_labels.append(labels[i])\n",
        "    \n",
        "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
        "\n",
        "    return (corpus, texts, final_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-19jVUuzsHe",
        "colab_type": "code",
        "outputId": "f942d1ad-e587-4f40-eac0-1e828d378a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# # Preprocess the corpus to get list of documents with stop words removed and containing only the words that are present in the vocab\n",
        "corpus = [preprocess(title) for title in titles_list]\n",
        "# # still contains all the dcuments, nothing is filtered\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # corpus[1] => ['fed', 'charles', 'plosser', 'sees', 'high', 'bar', 'change', 'pace', 'tapering']\n",
        "\n",
        "# # Remove docs that don't include any words in W2V's vocab\n",
        "corpus, titles_list, labels = filter_docs(corpus, titles_list, labels, lambda doc: has_vector_representation(model, doc))\n",
        "print(\"1st filter\",len(corpus), len(titles_list), len(labels))\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # corpus[1] => ['fed', 'charles', 'plosser', 'sees', 'high', 'bar', 'change', 'pace', 'tapering']\n",
        "# # titles_list[0] => Fed official says weak data caused by weather, should not slow taper\n",
        "# # titles_list[1] => Fed's Charles Plosser sees high bar for change in pace of tapering\n",
        "\n",
        "\n",
        "# # Filter out any empty docs\n",
        "corpus, titles_list, labels = filter_docs(corpus, titles_list, labels, lambda doc: (len(doc) != 0))\n",
        "print(\"2nd filter\",len(corpus), len(titles_list), len(labels))\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # titles_list[0] => Fed official says weak data caused by weather, should not slow taper\n",
        "\n",
        "x = []\n",
        "for doc in corpus: # append the vector for each document\n",
        "    x.append(document_vector(model, doc))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "241 docs removed\n",
            "1st filter 422178 422178 422178\n",
            "0 docs removed\n",
            "2nd filter 422178 422178 422178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md8PjWZJVqrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcabe3a1-66db-462f-e8fc-d054a7f25670"
      },
      "source": [
        "vectorsForEachDocument = np.array(x) # list to array\n",
        "labels = np.array(labels)\n",
        "labels = labels.reshape(labels.shape[0],1)\n",
        "vectorsForEachDocument.shape, labels.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((422178, 300), (422178, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T28DgqDk4M93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d95eae1b-3a3d-4b4e-c60a-c06551b9e625"
      },
      "source": [
        "# filter out data that has improper labels\n",
        "possibleLabels = [\"b\",\"t\",\"e\",\"m\"]\n",
        "finalLabels = []\n",
        "features = []\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] in possibleLabels:\n",
        "    finalLabels.append(labels[i])\n",
        "    features.append(vectorsForEachDocument[i])\n",
        "  if(len(finalLabels)==200000):\n",
        "    break\n",
        "\n",
        "\n",
        "features = np.array(features)\n",
        "labels = np.array(finalLabels)\n",
        "labels = labels.reshape(labels.shape[0],1)\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((200000, 300), (200000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAtrcPVp7rkr",
        "colab_type": "text"
      },
      "source": [
        "# **What all does the data contain at the moment.**\n",
        "\n",
        "---\n",
        "corpus: A list containing a list of words of which stop words are removed. Only the words that are present in the vocab are present here.<br>\n",
        "titles_list: It contains a list of all titles.<br>\n",
        "vectorsForEachDocument: It contains vectors for each document. This attribute can be used as the feature.<br>\n",
        "labels: Contains the category for respective title. This attribute can be used as the label.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK8LtVKbPGhz",
        "colab_type": "text"
      },
      "source": [
        "**Reduce the dimensions of the vectors.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzraKyNYOaAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdf542af-bb5a-4efa-d57b-79278aca322f"
      },
      "source": [
        "# tic = time.time()\n",
        "# vectorsForEachDocument =  TSNE(n_components=2).fit_transform(vectorsForEachDocument[0:25000])\n",
        "# toc = time.time()\n",
        "# print(\"Reduced to \"+str(vectorsForEachDocument.shape)+\" in \"+\"{0:.2f}\".format(toc-tic)+\" seconds\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reduced to (25000, 2) in 883.89 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5cM68QXWsBa",
        "colab_type": "text"
      },
      "source": [
        "# Time required for reducing dimensions\n",
        "\n",
        "---\n",
        "Reduced to (200, 100) in 9.75 seconds. <br>\n",
        "Reduced to (500, 2) in 3.06 seconds. <br>\n",
        "Reduced to (10000, 2) in 187.40 seconds. <br>\n",
        "Reduced to (25000, 2) in 855.39 seconds. <br>\n",
        "\n",
        "**Note:**<br>\n",
        "*Utilises 25GB RAM in google colab for reducing 100000 vectors to dimension of 100.* <br>\n",
        "*Same RAM utilisation for reducing 100000 vectors to dimension of 50 and same for 10 dimensions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1EyTWP7E9P",
        "colab_type": "text"
      },
      "source": [
        "Normalise the matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmmilelU5Mgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorsForEachDocument = vectorsForEachDocument/np.linalg.norm(vectorsForEachDocument, ord = 2, axis = 1, keepdims = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM1zaZDX6FRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0582fd94-7c19-432f-e53b-dfe65aac5940"
      },
      "source": [
        "# features[:5]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.2968750e-02,  9.5404729e-02, -3.0951606e-02, ...,\n",
              "         4.4660781e-02,  1.3774957e-01, -5.8464900e-02],\n",
              "       [-1.0681152e-04,  9.6664429e-02, -6.1843872e-02, ...,\n",
              "        -5.3588867e-02,  2.5878906e-02, -3.2043457e-03],\n",
              "       [-1.0219998e-02,  6.2337238e-02, -5.3702459e-02, ...,\n",
              "        -4.5288086e-02,  4.0269639e-02, -5.1662870e-02],\n",
              "       [-6.3273110e-03,  7.8562416e-02, -4.0201824e-02, ...,\n",
              "         5.7495117e-02,  3.4057617e-02, -7.8948975e-02],\n",
              "       [-6.7413330e-02,  9.9329628e-02, -1.1279297e-01, ...,\n",
              "        -5.0252280e-03,  5.0832111e-02, -4.2582195e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zokc8R5s7nH-",
        "colab_type": "text"
      },
      "source": [
        "Save data and labels as dataframe to downlaod it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc40d1IU7txC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "76736c8d-b5c4-40f9-ccb4-2275c4904ffd"
      },
      "source": [
        "normalisedData = pd.DataFrame.from_records(vectorsForEachDocument)\n",
        "# normalisedData.columns = range(1,3)\n",
        "normalisedData[\"labels\"] = labels\n",
        "normalisedData.to_csv('NormalisedData_30000*300.csv')\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-1033ad91746c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalisedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorsForEachDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# normalisedData.columns = range(1,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnormalisedData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalisedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NormalisedData_30000*300.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_records\u001b[0;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n\u001b[0;32m--> 426\u001b[0;31m                                dtype=dtype)\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[0;32m--> 436\u001b[0;31m                                  coerce_float=coerce_float)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_convert_object_array\u001b[0;34m(content, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7skjrdWIGoYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('NormalisedData_30000*300.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fLx5KcBoI8",
        "colab_type": "text"
      },
      "source": [
        "# Split the data\n",
        "---\n",
        "**Train Data**: Use 80% of the data for training purpose. <br>\n",
        "**Test Data**: Use 20% of the data for testing purpose. <br>\n",
        "**Features**: Use 300 dimensional vectors as features. It can be found in `vectorsForEachDocument`.<br>\n",
        "**Labels**: Use the categories as labels. I can be found in `labels`.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvfSgoRmBdIp",
        "colab_type": "code",
        "outputId": "4042e74d-271c-4c8d-ad61-f65aedf37e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train = int((80/100)*len(features))\n",
        "# train = 50000\n",
        "# test = len(features)-train\n",
        "trainFeatures, testFeatures = features[:train], features[train:]\n",
        "trainLabels, testLabels = labels[:train], labels[train:]\n",
        "trainFeatures.shape, trainLabels.shape, testFeatures.shape, testLabels.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((160000, 300), (160000, 1), (40000, 300), (40000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMISgZnDsB6",
        "colab_type": "text"
      },
      "source": [
        "**Lengths of Training and test data** <br>\n",
        "Training Set contains 337742 records.<br>\n",
        "Test Set contains 84436 records.<br><br>\n",
        "**Shapes of the data** <br>\n",
        "trainFeatures: (20000,2) <br>\n",
        "trainLabels: (5000,1) <br>\n",
        "testFeatures: (20000,2) <br>\n",
        "testLabels: (5000,1) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60mKGCXHvx8",
        "colab_type": "text"
      },
      "source": [
        "# **Train the logistic regression model** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87M4P4AJHccd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tic = time.time()\n",
        "logistic_Regression = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\")\n",
        "logistic_Regression.fit(trainFeatures,trainLabels)\n",
        "Y_predict = logistic_Regression.predict(testFeatures)\n",
        "print(accuracy_score(testLabels,Y_predict))\n",
        "toc = time.time()\n",
        "print(\"Time taken:\"+str(toc-tic)+\" seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dB0ddhLqZux",
        "colab_type": "text"
      },
      "source": [
        "# **Outcomes of the training** <br>\n",
        "\n",
        "---\n",
        "1.   With Train(9000) and test(1000) and 300 dim vectors: Accuracy=65.2% in 80 seconds\n",
        "2.   With Train(20000) and test(5000) and 2 dim vectors: Accuracy=61.3% in 6 seconds\n",
        "3.   With Train(24000) and test(6000) and 300 dim vectors: Accuracy=73% in 11 seconds\n",
        "4.   With Train(80000) and test(20000) and 300 dim vectors: Accuracy=78% in 48 seconds\n",
        "5.   With Train(160000) and test(40000) and 300 dim vectors: Accuracy=% in  seconds\n",
        "\n"
      ]
    }
  ]
}