{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsDataClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarsk98/NewsDataClassification/blob/development/NewsDataClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhxuk-S5nrFa",
        "colab_type": "code",
        "outputId": "eee0a99b-bb5f-4adb-bc1a-a7a1155d96c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-01 10:46:02--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.176.181\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.176.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "        GoogleNews-  28%[====>               ] 442.47M  33.9MB/s    eta 36s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtVE_u-I0m91",
        "colab_type": "code",
        "outputId": "d808fbdf-23e5-457c-d85e-9562655a2915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/omkarsk98/NewsDataClassification.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NewsDataClassification'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/17)\u001b[K\rremote: Compressing objects:  11% (2/17)\u001b[K\rremote: Compressing objects:  17% (3/17)\u001b[K\rremote: Compressing objects:  23% (4/17)\u001b[K\rremote: Compressing objects:  29% (5/17)\u001b[K\rremote: Compressing objects:  35% (6/17)\u001b[K\rremote: Compressing objects:  41% (7/17)\u001b[K\rremote: Compressing objects:  47% (8/17)\u001b[K\rremote: Compressing objects:  52% (9/17)\u001b[K\rremote: Compressing objects:  58% (10/17)\u001b[K\rremote: Compressing objects:  64% (11/17)\u001b[K\rremote: Compressing objects:  70% (12/17)\u001b[K\rremote: Compressing objects:  76% (13/17)\u001b[K\rremote: Compressing objects:  82% (14/17)\u001b[K\rremote: Compressing objects:  88% (15/17)\u001b[K\rremote: Compressing objects:  94% (16/17)\u001b[K\rremote: Compressing objects: 100% (17/17)\u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 5), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8wVHW5d00z1",
        "colab_type": "code",
        "outputId": "3ec65733-2058-4b34-f082-3a027eed50ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd NewsDataClassification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NewsDataClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ivy1jcg1KVc",
        "colab_type": "code",
        "outputId": "fa395317-1210-49ae-ae5d-6e44575b3078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!git checkout development"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'development' set up to track remote branch 'development' from 'origin'.\n",
            "Switched to a new branch 'development'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV1FV6q2n61M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9-41Dib1B-w",
        "colab_type": "code",
        "outputId": "a5b73730-9c35-4627-f309-85218aeaad91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin  NewsDataClassification.ipynb  README.md\n",
            "index.py\t\t\t    News_Final.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ozUYJ3oXO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from gensim import models\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70GDVH6tL1r",
        "colab_type": "code",
        "outputId": "70728548-0464-4e9d-b8ba-6827aa0909c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# read csv\n",
        "main_data = pd.read_csv('News_Final.csv')\n",
        "# read titles from it\n",
        "article_titles = main_data['TITLE']\n",
        "# Create a list of strings, one for each title\n",
        "titles_list = [title for title in article_titles]\n",
        "# form a single string fro the list of strings\n",
        "big_title_string = ' '.join(titles_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJDZ2gA83IWg",
        "colab_type": "code",
        "outputId": "896586bc-c916-4183-8e1f-d5fa1908c834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OIrcSbBuOWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the string into words\n",
        "tokens = word_tokenize(big_title_string)\n",
        "\n",
        "# Remove non-alphabetic tokens, such as punctuation\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if not word in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55YAVl-Oua1m",
        "colab_type": "code",
        "outputId": "a1a6f694-b78b-47b0-d3fb-4def68df3691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Load word2vec model (trained on an enormous Google corpus)\n",
        "model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n",
        "# Check dimension of word vectors\n",
        "print(\"Dimensions of the model\",model.vector_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dimensions of the model 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa1UoGCuuxsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
        "vector_list = [model[word] for word in words if word in model.vocab]\n",
        "# Create a list of the words corresponding to these vectors\n",
        "words_filtered = [word for word in words if word in model.vocab]\n",
        "\n",
        "# Zip the words together with their vector representations\n",
        "word_vec_zip = zip(words_filtered, vector_list)\n",
        "\n",
        "# Cast to a dict so we can turn it into a DataFrame\n",
        "word_vec_dict = dict(word_vec_zip)\n",
        "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "\n",
        "# save dataframe to csv just for reference\n",
        "df.to_csv('Words_vs_Vectors.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOoqYNI3KuXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Words_vs_Vectors.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz2aZ6OMRU3z",
        "colab_type": "code",
        "outputId": "5106189a-a334-4a4b-b1b3-6404214fb42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of words found:\", len(df))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words found: 35133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_h-pzrEzhNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using some pre built functions\n",
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in model.vocab]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
        "# Here, we need each document to remain a document \n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# Function that will help us drop documents that have no word vectors in word2vec\n",
        "def has_vector_representation(word2vec_model, doc):\n",
        "    \"\"\"check if at least one word of the document is in the\n",
        "    word2vec dictionary\"\"\"\n",
        "    return not all(word not in word2vec_model.vocab for word in doc)\n",
        "\n",
        "# Filter out documents\n",
        "def filter_docs(corpus, texts, condition_on_doc):\n",
        "    \"\"\"\n",
        "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
        "    \"\"\"\n",
        "    number_of_docs = len(corpus)\n",
        "\n",
        "    if texts is not None:\n",
        "        texts = [text for (text, doc) in zip(texts, corpus)\n",
        "                 if condition_on_doc(doc)]\n",
        "\n",
        "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
        "\n",
        "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
        "\n",
        "    return (corpus, texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-19jVUuzsHe",
        "colab_type": "code",
        "outputId": "b1f4c096-85f6-4dab-d010-1925d3650909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# # Preprocess the corpus to get list of documents with stop words removed and containing only the words that are present in the vocab\n",
        "# corpus = [preprocess(title) for title in titles_list]\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # corpus[1] => ['fed', 'charles', 'plosser', 'sees', 'high', 'bar', 'change', 'pace', 'tapering']\n",
        "\n",
        "# # Remove docs that don't include any words in W2V's vocab\n",
        "# corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # corpus[1] => ['fed', 'charles', 'plosser', 'sees', 'high', 'bar', 'change', 'pace', 'tapering']\n",
        "# # titles_list[0] => Fed official says weak data caused by weather, should not slow taper\n",
        "# # titles_list[1] => Fed's Charles Plosser sees high bar for change in pace of tapering\n",
        "\n",
        "\n",
        "# # Filter out any empty docs\n",
        "# corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))\n",
        "# # corpus[0] => ['fed', 'official', 'says', 'weak', 'data', 'caused', 'weather', 'slow', 'taper']\n",
        "# # titles_list => Fed official says weak data caused by weather, should not slow taper\n",
        "\n",
        "x = []\n",
        "for doc in corpus: # append the vector for each document\n",
        "    x.append(document_vector(model, doc))\n",
        "    \n",
        "vectorsForEachDocument = np.array(x) # list to array\n",
        "len(vectorsForEachDocument)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-05d49ec9cdd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# append the vector for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luTUeF4L3sJo",
        "colab_type": "code",
        "outputId": "c413c778-1af8-40b5-b653-e4fd11e4c1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# drop all rows that dont have category\n",
        "main_data = main_data.drop(main_data[main_data['CATEGORY'].isna()].index)\n",
        "main_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(422398, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAtrcPVp7rkr",
        "colab_type": "text"
      },
      "source": [
        "**What all does the data contain at the moment.**\n",
        "\n",
        "---\n",
        "corpus: A list containing a list of words of which stop words are removed. Only the words that are present in the vocab are present here.<br>\n",
        "titles_list: It contains a list of all titles.<br>\n",
        "vectorsForEachDocument: It contains vectors for each document.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miLBT7ZacejD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize t-SNE\n",
        "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
        "\n",
        "# Use only 400 rows to shorten processing time\n",
        "tsne_df = tsne.fit_transform(df[:400])\n",
        "tsne_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}