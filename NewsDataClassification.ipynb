{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsDataClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarsk98/NewsDataClassification/blob/development/NewsDataClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUJRD-W05nMs",
        "colab_type": "text"
      },
      "source": [
        "Download the repo for original and raw csv data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtVE_u-I0m91",
        "colab_type": "code",
        "outputId": "d5f3f9dd-b5f4-4a9a-a82c-791b65ebf799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/omkarsk98/NewsDataClassification.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NewsDataClassification'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/53)\u001b[K\rremote: Counting objects:   3% (2/53)\u001b[K\rremote: Counting objects:   5% (3/53)\u001b[K\rremote: Counting objects:   7% (4/53)\u001b[K\rremote: Counting objects:   9% (5/53)\u001b[K\rremote: Counting objects:  11% (6/53)\u001b[K\rremote: Counting objects:  13% (7/53)\u001b[K\rremote: Counting objects:  15% (8/53)\u001b[K\rremote: Counting objects:  16% (9/53)\u001b[K\rremote: Counting objects:  18% (10/53)\u001b[K\rremote: Counting objects:  20% (11/53)\u001b[K\rremote: Counting objects:  22% (12/53)\u001b[K\rremote: Counting objects:  24% (13/53)\u001b[K\rremote: Counting objects:  26% (14/53)\u001b[K\rremote: Counting objects:  28% (15/53)\u001b[K\rremote: Counting objects:  30% (16/53)\u001b[K\rremote: Counting objects:  32% (17/53)\u001b[K\rremote: Counting objects:  33% (18/53)\u001b[K\rremote: Counting objects:  35% (19/53)\u001b[K\rremote: Counting objects:  37% (20/53)\u001b[K\rremote: Counting objects:  39% (21/53)\u001b[K\rremote: Counting objects:  41% (22/53)\u001b[K\rremote: Counting objects:  43% (23/53)\u001b[K\rremote: Counting objects:  45% (24/53)\u001b[K\rremote: Counting objects:  47% (25/53)\u001b[K\rremote: Counting objects:  49% (26/53)\u001b[K\rremote: Counting objects:  50% (27/53)\u001b[K\rremote: Counting objects:  52% (28/53)\u001b[K\rremote: Counting objects:  54% (29/53)\u001b[K\rremote: Counting objects:  56% (30/53)\u001b[K\rremote: Counting objects:  58% (31/53)\u001b[K\rremote: Counting objects:  60% (32/53)\u001b[K\rremote: Counting objects:  62% (33/53)\u001b[K\rremote: Counting objects:  64% (34/53)\u001b[K\rremote: Counting objects:  66% (35/53)\u001b[K\rremote: Counting objects:  67% (36/53)\u001b[K\rremote: Counting objects:  69% (37/53)\u001b[K\rremote: Counting objects:  71% (38/53)\u001b[K\rremote: Counting objects:  73% (39/53)\u001b[K\rremote: Counting objects:  75% (40/53)\u001b[K\rremote: Counting objects:  77% (41/53)\u001b[K\rremote: Counting objects:  79% (42/53)\u001b[K\rremote: Counting objects:  81% (43/53)\u001b[K\rremote: Counting objects:  83% (44/53)\u001b[K\rremote: Counting objects:  84% (45/53)\u001b[K\rremote: Counting objects:  86% (46/53)\u001b[K\rremote: Counting objects:  88% (47/53)\u001b[K\rremote: Counting objects:  90% (48/53)\u001b[K\rremote: Counting objects:  92% (49/53)\u001b[K\rremote: Counting objects:  94% (50/53)\u001b[K\rremote: Counting objects:  96% (51/53)\u001b[K\rremote: Counting objects:  98% (52/53)\u001b[K\rremote: Counting objects: 100% (53/53)\u001b[K\rremote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 53 (delta 23), reused 15 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LINs-Fbc5ukI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Go the directory and checkout to the required folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8wVHW5d00z1",
        "colab_type": "code",
        "outputId": "5d18a2d3-822b-4ce8-aeb1-523061c83d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd NewsDataClassification\n",
        "!git checkout development"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NewsDataClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZiinfyg54Ta",
        "colab_type": "text"
      },
      "source": [
        "Download trained model and unzip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTChM-bd_umC",
        "colab_type": "code",
        "outputId": "973ae79c-fb49-4cdc-8bbb-b070b743419e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-10 11:46:46--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.92.245\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.92.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.3MB/s    in 98s     \n",
            "\n",
            "2019-10-10 11:48:25 (16.0 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5leTMSU5-wT",
        "colab_type": "text"
      },
      "source": [
        "Download nltk libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJDZ2gA83IWg",
        "colab_type": "code",
        "outputId": "0485cbed-e54d-46dd-b6a1-aba1cefbc46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55YAVl-Oua1m",
        "colab_type": "code",
        "outputId": "4e85ef2d-89dd-43e9-f8df-6472ef845133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Load word2vec model (trained on Google's corpus)\n",
        "model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n",
        "# Check dimension of word vectors\n",
        "print(\"Dimensions of the model\",model.vector_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dimensions of the model 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og2mnsbQ6EzL",
        "colab_type": "text"
      },
      "source": [
        "import all required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ozUYJ3oXO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from gensim import models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score ,confusion_matrix\n",
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8AmIVB6KPk",
        "colab_type": "text"
      },
      "source": [
        "Read the raw data and set max records to be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70GDVH6tL1r",
        "colab_type": "code",
        "outputId": "91eb6cf7-aee2-4620-d2b4-402742a63c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# read csv\n",
        "main_data = pd.read_csv('News_Final.csv')\n",
        "# read titles from it\n",
        "article_titles = main_data['TITLE']\n",
        "labels = main_data[\"CATEGORY\"]\n",
        "\n",
        "# Create a list of strings, one for each title\n",
        "titles_list = [title for title in article_titles]\n",
        "# form a single string fro the list of strings\n",
        "big_title_string = ' '.join(titles_list)\n",
        "\n",
        "# define total records to be considered for analysis\n",
        "total = 50000\n",
        "# 422178 total records as max value"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzMgB8HD6dbT",
        "colab_type": "text"
      },
      "source": [
        "Tokenise all words and get stop words for english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WqUDhgQJaIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the string into words\n",
        "tokens = word_tokenize(big_title_string)\n",
        "\n",
        "# Remove non-alphabetic tokens, such as punctuation\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo5Yg-u06i7I",
        "colab_type": "text"
      },
      "source": [
        "Define all the function that can be used for later stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_h-pzrEzhNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in model.vocab]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
        "# Here, we need each document to remain a document \n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# Function that will help us drop documents that have no word vectors in word2vec\n",
        "def has_vector_representation(word2vec_model, doc):\n",
        "    \"\"\"check if at least one word of the document is in the\n",
        "    word2vec dictionary\"\"\"\n",
        "    return not all(word not in word2vec_model.vocab for word in doc)\n",
        "\n",
        "# Filter out documents\n",
        "def filter_docs(corpus, texts, labels, condition_on_doc):\n",
        "    \"\"\"\n",
        "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
        "    \"\"\"\n",
        "    number_of_docs = len(corpus)\n",
        "    \n",
        "    if texts is not None:\n",
        "        texts = [text for (text, doc) in zip(texts, corpus)\n",
        "                 if condition_on_doc(doc)]\n",
        "    \n",
        "\n",
        "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
        "    \n",
        "    final_labels = []\n",
        "    for i in range(len(corpus)):\n",
        "      if condition_on_doc(corpus[i]):\n",
        "        final_labels.append(labels[i])\n",
        "    \n",
        "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
        "\n",
        "    return (corpus, texts, final_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNpB7L-a6njk",
        "colab_type": "text"
      },
      "source": [
        "Remove stop words, non vocab words, empty docs and prepare vector for each title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-19jVUuzsHe",
        "colab_type": "code",
        "outputId": "0c996c9b-af9b-4ab7-cc6b-1f2da87c5b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# # Preprocess the corpus to get list of documents with stop words removed and containing only the words that are present in the vocab\n",
        "corpus = [preprocess(title) for title in titles_list]\n",
        "# # still contains all the documents, nothing is filtered\n",
        "\n",
        "# # Remove docs that don't include any words in W2V's vocab\n",
        "corpus, titles_list, labels = filter_docs(corpus, titles_list, labels, lambda doc: has_vector_representation(model, doc))\n",
        "print(\"1st filter: Length of corpus:\"+str(len(corpus))+\", Length of titles_list:\"+ str(len(titles_list))+\", Length of labels:\"+str(len(labels)))\n",
        "\n",
        "# # Filter out any empty docs\n",
        "corpus, titles_list, labels = filter_docs(corpus, titles_list, labels, lambda doc: (len(doc) != 0))\n",
        "print(\"2nd filter: Length of corpus:\"+str(len(corpus))+\", Length of titles_list:\"+ str(len(titles_list))+\", Length of labels:\"+str(len(labels)))\n",
        "\n",
        "x = []\n",
        "for doc in corpus: # append the vector for each document\n",
        "    x.append(document_vector(model, doc))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "241 docs removed\n",
            "1st filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178\n",
            "0 docs removed\n",
            "2nd filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Tp1BD1ibrf",
        "colab_type": "text"
      },
      "source": [
        "# **After removing stop words and empty docs** <br>\n",
        "\n",
        "---\n",
        "241 docs removed <br>\n",
        "1st filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178 <br>\n",
        "0 docs removed <br>\n",
        "2nd filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178 <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md8PjWZJVqrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3c43aa4-5560-4228-c49c-b065b4aeb9d1"
      },
      "source": [
        "vectorsForEachDocument = np.array(x) # list to array\n",
        "labels = np.array(labels)\n",
        "labels = labels.reshape(labels.shape[0],1)\n",
        "vectorsForEachDocument.shape, labels.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((422178, 300), (422178, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-pmoZA-of0k",
        "colab_type": "text"
      },
      "source": [
        "# **Vectors for each title** \n",
        "\n",
        "---\n",
        "A list of vectors of 300 dimensions each for all the titles and labels contain the respective labels<br>\n",
        "Shape of these vectors is (422178, 300) <br>\n",
        "Shape pf it respective labels (422178, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToFAFzWpPi5",
        "colab_type": "text"
      },
      "source": [
        "# **Filter improper labels**\n",
        "\n",
        "---\n",
        "Filter out the data that has improper labels. Labels should only be of the following types. <br>\n",
        "1. b: business\n",
        "2. t: technology\n",
        "3. e: entertainment\n",
        "4. m: health"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T28DgqDk4M93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e76c52e-27bb-439c-909e-26be9ea33536"
      },
      "source": [
        "# filter out data that has improper labels\n",
        "possibleLabels = [\"b\",\"t\",\"e\",\"m\"]\n",
        "finalLabels = []\n",
        "features = []\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] in possibleLabels:\n",
        "    finalLabels.append(labels[i])\n",
        "    features.append(vectorsForEachDocument[i])\n",
        "  if(len(finalLabels)==total):\n",
        "    break\n",
        "\n",
        "\n",
        "features = np.array(features)\n",
        "labels = np.array(finalLabels)\n",
        "labels = labels.reshape(labels.shape[0],1)\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 300), (50000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1O6Fg30g9j",
        "colab_type": "text"
      },
      "source": [
        "# **Create a dataframe to shuffle it**\n",
        "\n",
        "---\n",
        "Create dataframe to shuffle it and split it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc40d1IU7txC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26227549-2ed9-41a0-b0f9-3efa2ac65e0f"
      },
      "source": [
        "finalData = pd.DataFrame.from_records(features)\n",
        "finalData.columns = range(1,301)\n",
        "finalData[\"labels\"] = labels\n",
        "# finalData.to_csv('FinalData.csv')\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 300), (50000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID7XxZrx141C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40393d71-d9a6-4e6c-d7a5-ed257521d719"
      },
      "source": [
        "data = finalData.sample(frac=1) #shuffles the data\n",
        "labels = data[\"labels\"]\n",
        "labels = np.array(labels) \n",
        "labels = labels.reshape(labels.shape[0],1)\n",
        "del data[\"labels\"]\n",
        "features = np.array(data)\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 300), (50000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fLx5KcBoI8",
        "colab_type": "text"
      },
      "source": [
        "# Split the data\n",
        "---\n",
        "**Train Data**: Use 80% of the data for training purpose. <br>\n",
        "**Test Data**: Use 20% of the data for testing purpose. <br>\n",
        "**Features**: Use 300 dimensional vectors as features. It can be found in `vectorsForEachDocument`.<br>\n",
        "**Labels**: Use the categories as labels. I can be found in `labels`.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvfSgoRmBdIp",
        "colab_type": "code",
        "outputId": "d490bf2f-a8bc-497b-cef9-f0274e348fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train = int((80/100)*len(features))\n",
        "trainFeatures, testFeatures = features[:train], features[train:]\n",
        "trainLabels, testLabels = labels[:train], labels[train:]\n",
        "trainFeatures.shape, trainLabels.shape, testFeatures.shape, testLabels.shape"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((40000, 300), (40000, 1), (10000, 300), (10000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMISgZnDsB6",
        "colab_type": "text"
      },
      "source": [
        "**Shapes of the data** <br>\n",
        "trainFeatures: (160000,300) <br>\n",
        "trainLabels: (160000,1) <br>\n",
        "testFeatures: (40000,300) <br>\n",
        "testLabels: (40000,1) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60mKGCXHvx8",
        "colab_type": "text"
      },
      "source": [
        "# **Train the logistic regression model** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87M4P4AJHccd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "99317b70-519d-44b9-b554-ee43b1b1f34b"
      },
      "source": [
        "tic = time.time()\n",
        "logistic_Regression = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=1000)\n",
        "logistic_Regression.fit(trainFeatures,trainLabels)\n",
        "Y_predict = logistic_Regression.predict(testFeatures)\n",
        "print(str((accuracy_score(testLabels,Y_predict)*100))+\"%\")\n",
        "toc = time.time()\n",
        "print(\"Time taken:\"+str(toc-tic)+\" seconds\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "85.25%\n",
            "Time taken:11.742001056671143 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dB0ddhLqZux",
        "colab_type": "text"
      },
      "source": [
        "# **Outcomes of the training** <br>\n",
        "\n",
        "---\n",
        "|Train |Test |Dimensions |Accuracy |Time(sec) | Comments |\n",
        "|---|---|---|---|---|---|\n",
        "|40000|10000|300|85|12|Data randomly shuffled|\n",
        "|80000|20000|300|78|53||\n",
        "|40000|10000|300|74|12||\n",
        "|80000|20000|300|78|53||\n",
        "|24000|6000|300|73|11||\n",
        "|160000|40000|300|72|53||\n"
      ]
    }
  ]
}